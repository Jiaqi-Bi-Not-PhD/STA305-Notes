\documentclass[a4paper,11pt]{article}

\setlength{\oddsidemargin}{0 in}
\setlength{\evensidemargin}{0 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz-cd}
\usepackage{mathrsfs}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{enumitem}
\usepackage{yfonts}
\usepackage{ dsfont }
\usepackage{soul}
\usepackage{ulem,xpatch}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{listings}
%\usepackage[usenames,dvipsnames]{color}
\usepackage{setspace}
\usepackage[
    type={CC},
    modifier={by-nc-nd},
    version={4.0},
]{doclicense}
\usetikzlibrary{matrix,shapes,arrows,positioning,chains}
\usepackage{graphicx}



\newcommand{\soutthick}[1]{%
    \renewcommand{\ULthickness}{2.4pt}%
       \sout{#1}%
    \renewcommand{\ULthickness}{.4pt}% Resetting to ulem default
}




\title{\textbf{STA305 Design and Analysis of Experiments Lecture Notes (2021 Summer)}}

\author{Jiaqi Bi, University of Toronto}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}

\newtheorem{defn}[thm]{Definition}
\newtheorem{eg}[thm]{Example}
\newtheorem{ex}[thm]{Exercise}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{claim}[thm]{Claim}
\newtheorem{rmk}[thm]{Remark}

\newcommand{\ie}{\emph{i.e.} }
\newcommand{\cf}{\emph{cf.} }
\newcommand{\into}{\hookrightarrow}
\newcommand{\dirac}{\slashed{\partial}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\LieT}{\mathfrak{t}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\A}{\mathds{A}}

\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\ord}{ord}

\begin{document}
% Define block styles
\tikzset{
decision/.style={
    diamond,
    draw,
    text width=4em,
    text badly centered,
    inner sep=0pt
},
block/.style={
    rectangle,
    draw,
    text width=10em,
    text centered,
    rounded corners
},
cloud/.style={
    draw,
    ellipse,
    minimum height=2em
},
descr/.style={
    fill=white,
    inner sep=2.5pt
},
connector/.style={
    -latex,
    font=\scriptsize
},
rectangle connector/.style={
    connector,
    to path={(\tikztostart) -- ++(#1,0pt) \tikztonodes |- (\tikztotarget) },
    pos=0.5
},
rectangle connector/.default=-2cm,
straight connector/.style={
    connector,
    to path=--(\tikztotarget) \tikztonodes
}
}

\setstretch{1.1}
\begin{titlepage}
\end{titlepage}

\maketitle

\newpage

\begin{Large}
\noindent \textbf{Preface}
\end{Large}

\vspace{6pt}

\noindent This typesetting note is intended for academic communication and not-for-profit knowledge sharing. Any institutions or individuals \textbf{SHOULD NOT} reproduce or use this note in any form for operational or commercial purposes. This note contains some of the lecture notes instructed by Prof. Ramya Thinniyam and personal research notes. The author holds the right of the prosecution for any above behaviors. Moreover, this note should not be used as a substitution for the current user's attending lecture or textbook. I strongly recommend the user combine the note with your textbook and lecture to understand the content thoroughly. I hope anyone using this note can have a better understanding of the Design and Analysis of Experiments topic. Students using this copy should not conduct any academic infractions, including but not limited to reproducing, failure of citations, and using this copy in any forms of academic dishonesty mentioned by the University of Toronto. For specific details of academic integrity, please visit https://www.academicintegrity.utoronto.ca/. 

\doclicenseThis

\newpage

\tableofcontents

\newpage

\section{Lec 1 Introduction to Experiments and Types of Data}
\subsection{General Outline of an Experiment}
\begin{itemize}
\item An experiment is used to prove a scientific claim
\item Simplest Setup: 2 groups
\begin{itemize}
\item Apply a different Treatment to each group, i.e. Treatment and Control
\item Measure a variable of interest, i.e. Response
\end{itemize}
\item Ideal situation: the groups are identical in every possible way, except for the treatment
\item If the groups differ in the response, it must be due to the treatment variable. 
\end{itemize}
A flowchart that generally explain this: 

\begin{tikzpicture}
\matrix (m)[matrix of nodes, column  sep=4mm,row  sep=10mm, align=center, nodes={rectangle,draw, anchor=center} ]{
 &   |[block]| {2 Groups}              &  \\
    |[block]| {Treatment }               &    &    |[block]|{Control}                                   \\
    |[block]| {Response 1}          &     &      |[block]|{Response 2}                                  \\
  &  |[block]| {Responses defer due to the treatment variable}    &                                              \\
};
\path [>=latex,->] (m-1-2) edge (m-2-1);
\path [>=latex,->] (m-1-2) edge (m-2-3);
\path [>=latex,->] (m-2-1) edge (m-3-1);
\path [>=latex,->] (m-2-3) edge (m-3-3);
\path [>=latex,->] (m-3-1) edge (m-4-2);
\path [>=latex,->] (m-3-3) edge (m-4-2);



\end{tikzpicture}

Example of different treatments: Medicine \& Placebo
\begin{eg}
\normalfont
A research team wants to test their effect of a new medicine on the Migraine. They have the \colorbox{pink}{treatment group} of New Pill, and they have the \colorbox{pink}{control group} of Placebo. Then they will test migraine sufferers. The \colorbox{pink}{response} will be the pain level from 1 to 10 subjectively measured by patients themselves. The duration will be 2 hours after taking the pill. In this experiment, the \colorbox{pink}{nuisance variables} are: Age, Sleep hours, Diet, Severity of migraines... \colorbox{pink}{The treatment has two levels}: pill and placebo
\end{eg}
\newpage
\subsection{Steps of an Experimental Design}
\textbf{This would be useful for Final Project}

\begin{center}
	\includegraphics[width=1\textwidth]{"Pic 1".JPG}
\end{center}

\subsection{Definitions used in Statistical Inference}
\begin{itemize}
\item Population: Set of units that we are interested in studying. E.g. Group of people, objects...
\item Experimental Unit: The person or object on which the treatment is applied. Also called "case" or "Element" or "Subject" (When human unit). 
\item Sample: Subset of the population.
\item Variable: A measured characteristic of a population unit. E.g., Age, weight, pain scale... 
\item Statistical Inference: Estimate, prediction, or generalization about population based information from a sample. 
\end{itemize}

\subsection{Experimental Study vs. Observational Study}
Note this course we mainly focus on the experimental study. If you are interested in observational study you are recommended to take STA304. 

\textbf{Experimental Study}
\begin{itemize}
\item To assess the effect of a certain treatment or condition
\item First randomly assign subjects to treatment, then control the rest. Randomization is important. Experimental data comes from experiments, so there are eligible reasons to conclude \colorbox{pink}{causation}. 
\item Common in scientific and psychological research
\item \colorbox{pink}{Between Subjects Design}: Each experimental unit is assigned only one treatment
\item \colorbox{pink}{Within Subjects Design}: Each experimental unit is given all treatments
\end{itemize}
\textbf{Observational Study}
\begin{itemize}
\item No randomization or controlling
\item Data is collected as it comes (survey data), results from observational studies. 
\item Causation cannot be concluded from observational data since there may be other "confounding/lurking variables" (that were not controlled for) that is causing the relationship. 
\item Common in data mining, economic, and sociological research... 
\end{itemize}
Note that an experiment may be hard to conduct due to ethical reasons, costs... 
\subsection{Types of Variables}
There are two types of variables, which are $X$ and $Y$ such that $Y$ denotes the response, that is measured by the researcher, and influenced by other variables. $X$ is known as predictor or explanatory variable that affect the response variable, which is manipulated by researchers. 

Variables can be:
\begin{itemize}
\item Quantitative: A variable is quantitative if it takes on numerical values for which arithmetic operations make sense. Ex: Height, marks, incomes... This type of variables can be discrete or continuous. 
\item Discrete: Variable can take on any one of a finite or countable list of values ($\Z$) (there are gaps in the possible values). Ex: Number of heads in 10 tosses of a coin. 
\item Continuous: Variable can take on any value possible in an interval (uncountable). Ex: Time to complete a marathon... (There are infinitely many numbers in $\R$)
\item Qualitative/Categorical variable: Categories, cannot be measured on numerical scales. Ex: Gender, smoking status (No, Used to, Yes)... 
\end{itemize}

Variables can be measured as either qualitative or quantitative and further categorized as follows:

\textbf{Categorical:}
\begin{enumerate}
\item Nominal: Categorize units into distinct classes. 
\begin{itemize}
\item Unordered categories
\item Numerical computations are not applicable
\item Ex: Gender, POSt, Favorite colors...
\item When controlled by researcher, called \colorbox{pink}{Factor}
\end{itemize}
\item Ordinal: Ordered categories without natural units/distance metric
\begin{itemize}
\item Natural ordering to the categories; not just the names of the categories differ
\item Professional rank, grades, satisfications... 
\end{itemize}
\end{enumerate}
\textbf{Quantitative:}
\begin{enumerate}
\setcounter{enumi}{2}
\item Interval: Numerical measurements which allow for degree of difference between values. 
\begin{itemize}
\item Distance is consistent but ratios are meaningless
\item Does not have a true 0 measure (0 is arbitrary)
\item Ex: Temperature ($^{\circ}$C), dates...
\end{itemize}
\item Ratio: Numerical measurements on which a unique and non-arbitrary zero value exists
\begin{itemize}
\item Ratios are meaningful, sensible to carry our multiplication/division
\item Ex: Temperature (K), length, time duration...
\end{itemize}
\end{enumerate}

When an experiment has controls, call $X$ a \colorbox{pink}{factor} and its categories the \colorbox{pink}{factor levels}. Single-Factor Experiment has one factor with levels $\{a_1, a_2, a_3,...\}$. Two-Factor Experiment has Two Factors (Factor A and B) with their own levels. A \colorbox{pink}{treatment} is a combination of factor levels. 
\newpage
\begin{eg}
\normalfont
Example of Effects of Putting Sugar in Coffee

	\includegraphics[width=1\textwidth]{"Pic 2".JPG}

\begin{enumerate}
\item For the above study, identify the following: 
\begin{itemize}
\item Population- \textcolor{red}{All coffee drinkers}
\item Experimental unit- \textcolor{red}{A human participant}
\item Variables (Which is response/predictor, quantitative/qualitative?)- \textcolor{red}{$X$: Drink with sugar or not, $Y$: Respiratory gas exchange and their pulse (Quantitative)}
\item Sample- \textcolor{red}{12 participants}
\item A possible Statistical Inference- \textcolor{red}{Generalization on effects of drinking coffee with sugar}
\end{itemize}
\item Is the study observational or \colorbox{green}{experimental}? Justify. \textcolor{red}{The data were collected by an experiment, it has controls of giving specific amount of coffee and sugar. It is a within subjects design. Treatments are assigned.}
\item What are the factors and levels? Describe the treatments and how many there are? \textcolor{red}{Factor: Type of drink. Factor Levels (3 levels/treatments): Coffee, Coffee with sugar, sugar only) (categorical)}
\item Name a confounding variable that could be present in this study. \textcolor{red}{The order of treatments applied}
\end{enumerate}
\end{eg}

\begin{eg}
\normalfont
\textbf{Effects of Teaching Style}
Suppose you wish to conduct an experiment to see if teaching style (lecturer, facilitator, online technology) and the amount of lecture time (2 hours, 3 hours/week) affects the learning outcome of students in a large university course. 

\begin{enumerate}
\item What are the factors and levels? \textcolor{red}{Teaching styles (Lecture, Facilitator, Online Tech); The amount of lecture time (2 hours, 3 hours)}
\item Treatments? \textcolor{red}{(Lecture \& 2 hours, Lecture \& 3 hours, Facilitator \& 2 hours, Facilitator \& 3 hours, Tech \& 2 hours, Tech \& 3 hours) Total 6 treatments}
\end{enumerate}
\end{eg}

\subsection{Crossed vs. Nested Factors}
When \colorbox{pink}{all combinations} of factors are possible, the experiment is said to be fully \colorbox{pink}{crossed}. Easier to analyze. 

When each level of one factor occurs with a unique set of levels of the other factor, the design is called \colorbox{pink}{nested}. Hierarchical Design. 

\begin{center}
	\includegraphics[width=1\textwidth]{"Pic 3".JPG}
\end{center}

\subsection{Nuisance Variables}
Variables other than the treatment condition that influence the response variable. 

Ex: Teaching Styles: 
\begin{itemize}
\item Different lecturers for each section
\item Age of students
\item Time of day for lecture section, etc...
\end{itemize}

We use \colorbox{pink}{Control, Blocking, Randomization, Replication} to deal with nuisance variables. 

\textbf{Control}: Keep the nuisance variable constant throughout all treatment conditions (no longer a variable)

Ex: Teaching Styles: Same lecturer for each style; Same/Similar lecture times for each style...

\textbf{Blocking}: If the nuisance variable cannot be controlled, but can be observed, use Blocking to ensure each treatment has an equal amount of the variable. 

Ex: Assign students to styles so each style has same age distribution. 
\begin{center}
	\includegraphics[width=1\textwidth]{"Pic 4".JPG}
\end{center}

\textbf{Randomization}: If the nuisance variable cannot be controlled or observed, use Randomization to spread out the variables and reduce the chance of confounding. 

Ex: Teaching Styles: After blocking, randomly assign students to styles/sections. 

\textbf{Replication}: Should not conduct the experiment with only one observation in each treatment. The resulting effect may be due to the experimental unit, and not the treatment. Cannot estimate variance within a specific treatment if it has only one observation. Use of replicates allows us to estimate \colorbox{pink}{experimental error}, which is used to determine if we have statistically significant results. 

Replication can occur at two different levels: 
\begin{itemize}
\item \textbf{Treatment Level}: We take more than one observation in each treatment. 
\item \textbf{Experiment Level}: We wish to replicate the entire experiment to ensure results were not due other features inherent in the experiment, and so we can generalize to other populations. 
\end{itemize}

Ex: Teaching Styles: Many students with similar characteristics (treatments) in each class; Another university could replicate the study.

\subsection{Confounding}
Nuisance variables that are systematically related to the treatments. Confounding variables can alter the effect of treatments even if non-systematic relationship to treatments, accidental effects can increase variability of responses and mask treatment effects. 

Ex: Teaching Styles: Certain styles may be taught at different times of the day. For example, tech section always taught in night and lecture style during day. Differences cannot be uniquely credited to lecture styles. 

Randomization decreases the chance that factors not accounted for in the design of the experiment will be confounded with the treatments. 

\subsection{Blinding}
We use this when participants may be influenced once they know their treatment. \textbf{Placebo}. 

\textbf{Double blind}: Both subjects and researcher are unaware of treatment assignment. 

\subsection{Balance}
Assign the same number of experimental units to each treatment, call it \colorbox{pink}{Balanced Design}. Balanced and unbalanced designs dealt with differently (calculations, etc...). Balanced design can become unbalanced easily- missing data, destroyed measurements, etc...

\newpage

\section{Lec 2 Review of Hypothesis Testing and One-Way ANOVA with 2 Levels}
\subsection{Single Factor Analysis with 2 levels}
\begin{itemize}
\item 1 Factor with 2 levels (2 different treatments, treatment/control, etc. )
\item 2 treatments in total
\item Want to determine if there is a ``treatment effect" (if response varies by treatment)
\item Test for differences in means of the 2 populations: sample means are not enough; account for variability within each treatment group
\end{itemize}

\subsection{Review of Hypothesis Testing}
\begin{defn}
\normalfont
A Hypothesis Test is a formal statistical test that is performed to decide whether a statement about a set of parameter(s) is reasonable (to make an inference about the value of a parameter and how it relates to a specified/hypothesized numerical value). 
\end{defn}
\textbf{Elements of a Hypothesis Test}
\begin{enumerate}
\item $H_0$ is Null Hypothesis and $H_a$ is Alternative Hypothesis. 
\item Test Statistic: A statistic (function of the data) that involves the parameter value and has a known distribution under $H_0$.
\item Distribution under $H_0$: Distribution of test statistic assuming that $H_0$ is true
\item P-value: Probability of observing a test statistic as or more extreme (more contradictory to $H_0$ i.e. favorable to $H_a$) than already observed if the null hypothesis is true. Gives a measure of strength of evidence against $H_0$. Small P-value, test statistic value is unlikely if $H_0$ is true (contradiction). Large p-value, test statistic value is likely if $H_0$ is true (No contradiction)
\item Conclusion: Reject $H_0$ (and favour $H_a$) or Fail to Reject $H_0$. This leads to a practical conclusion about the population(s)/parameter value(s). When $H_0$ is rejected, we say the test is \colorbox{pink}{statistically significant}, or there is a \colorbox{pink}{statistically significant difference}. 
\end{enumerate}
\subsection{Review: Two Sample T-Tests}
\begin{defn}
\normalfont
Interested in comparing two population means when we have small sample sizes. Suppose we have $X_1,...,X_{n1}$ a random sample from population 1 and $Y_1,...,Y_{n2}$ a random sample from population 2. 

$H_0$ is $\mu_1-\mu_2=D_0$, note that this $D_0$ is usually 0 but not always (When treatment means are equal). We have $H_a:\mu_1-\mu_2\neq D_0$ OR $\underbrace{Ha: \mu_1-\mu_2<D_0 \text{ OR } H_a:\mu_1-\mu_2>D_0}_\text{One-tail test}$

\textbf{Assumptions}: 
\begin{enumerate}
\item Two samples are $iid$ from Normal populations
\item Two samples are independent from each other
\end{enumerate}
\end{defn}
The test statistic is
$$t=\frac{(\bar{x}-\bar{y})-D_0}{se(\bar{x}-\bar{y})}$$
where $se$ is the standard error: estimated standard deviation. 
\subsection{Pooled T-Test}
\begin{defn}
\normalfont
Applicable when population variances can be assumed to be equal. I.e. Population 1: $X_1,...,X_{n1}\stackrel{iid}{\sim}N(\mu_1, \sigma_1^2)$ and Population 2: $Y_1,...,Y_{n2}\stackrel{iid}{\sim}N(\mu_2,\sigma_2^2)$ such that $\sigma_1=\sigma_2$. 

\textbf{Assumptions}:
\begin{enumerate}
\item Two samples are random samples from the target populations and the samples are independent from each other. 
\item The populations have equal standard deviations ($\sigma_1=\sigma_2$)
\item Two samples are (approximately) Normal
\end{enumerate}
If we can assume that the variances are equal, we use the \colorbox{pink}{pooled estimate of the variance}: 
$$s_p^2=\frac{\overbrace{(n_1-1)}^\text{df of $s_1^2$}s_1^2+\overbrace{(n_2-1)}^\text{df of $s_2^2$}s_2^2}{\underbrace{n_1+n_2-2}_\text{total df}}$$

\textbf{Hypothesis for Pooled T-Tests}: 

$H_0:\mu_1-\mu_2=D_0$ vs. 
\begin{enumerate}
\item $H_a:\mu_1-\mu_2\neq D_0$ OR
\item $H_a:\mu_1-\mu_2<D_0$ OR
\item $H_a:\mu_1-\mu_2>D_0$. 
\end{enumerate}

\textbf{Test Statistic for Pooled T-Tests}:

$$t=\frac{(\bar{x}-\bar{y})-D_0}{\sqrt{s_p^2(\frac{1}{n_1}+\frac{1}{n_2})}}\sim t_{n_1+n_2-2}\text{ under }H_0$$

\begin{enumerate}
\item $p=2P(t_{n_1+n_2-2}>|t|)$ OR
\item $p=P(t_{n_1+n_2-2}<t)$ OR
\item $p=P(t_{n_1+n_2-2}>t)$
\end{enumerate}

Conclusion: Reject $H_0$ is $p<\alpha$ and failed to reject otherwise. 
\end{defn}

\subsection{Welch's T-Test Using Satterthwaite Approximation}
\begin{defn}
\normalfont
Used when population variances are not equal or inconclusive evidence about equality of variances. 

\textbf{Assumptions}: Two samples are random from the target populations and the samples are independent from each other; Two samples are Normal. 

\textbf{Test Statistic for Welch's T-Test}
$$t=\frac{(\bar{x}-\bar{y})-D_0}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}\sim t_{\mu}\text{ under }H_0$$
where
$$\mu=\frac{(s_1^2/n_1+s_2^2/n_2)^2}{\frac{(s_1^2/n_1)^2}{n_1-1}+\frac{(s_2^2/n_2)^2}{n_2-1}}$$

The degree of freedom is calculated by Satterthwaite Approximation. Note that $\mu$ may not be an integer, \colorbox{pink}{round down to the nearest integer}. 
\end{defn}
\newpage
\begin{eg}
\normalfont
Example of Arthritis Clinical Study

\begin{center}
	\includegraphics[width=1\textwidth]{"pic 5".JPG}
\end{center}

There are 293 patients who suffering from RA, they are firstly measured of the initial pain. Then 146 are taking Auranofin and 147 are taking Placebo. After 6 months, we measure their pains and compare. There is a repeated measurement which is before pain and after pain. This is a between subjects design. The factor of the experiment is treatment (medicine). It has 2 levels that are Auranofin and Placebo. The responses are Pain before and Pain after 6 months. The researcher will compare the difference. $\mu_A=$ mean difference in pain (before-after) for Auranofin users. $\mu_P$=mean difference in pain (before-after) for placebo users. 

Question: Does the Auranofin therapy improve the pain? i.e., Is $\mu_A>\mu_P$? Note that $\mu$ is a mean difference, not the pain level! 
\newpage
The R Output is following: 

\begin{center}
	\includegraphics[width=1\textwidth]{"pic 6".JPG}
\end{center}
\begin{center}
	\includegraphics[width=1\textwidth]{"pic 7".JPG}
\end{center}
\begin{center}
	\includegraphics[width=1\textwidth]{"pic 8".JPG}
\end{center}

Using hand calculation: 

Pooled T-Test: $H_0: \mu_A=\mu_P$ vs. $H_a: \mu_A>\mu_P$, we have data: 

\begin{center}
\begin{tabular}{ |c|c|c|c|c| } 
\hline
Treatment & $n$ & $\bar{y}$ & $s^2$ \\
\hline
Auranofin & $146$ & $0.6575$ & $1.1785$ \\ 
Placebo & $147$ & $0.3401$ & $0.9383$ \\ 
\hline
\end{tabular}
\end{center}

With the data above, we have 
$$s_{\text{pooled}}^2=\frac{145(1.1785)+146(0.9383)}{293-2}\doteq 1.0660$$
$$t=\frac{(\bar{y}_A-\bar{y}_P)-0}{\sqrt{s_{\text{pooled}}^2(\frac{1}{n_A}+\frac{1}{n_P})}}=\frac{0.6575-0.3401}{\sqrt{1.0660(\frac{1}{146}+\frac{1}{147})}}\doteq 2.64\sim t_{291}\text{ under }H_0$$

By using T-Table: $p=P(t_{291}>2.64)=0.0041<0.05$. 

Since $p<0.05$, we reject $H_0$. Strong evidence to conclude that Auranofin therapy helps improve pains. 

Using R, we can get the exact p-value that the 2 sided $p=0.0087$, so the exact p value will be $p=P(t_{291}>2.64)=\frac{0.0087}{2}=0.00435$. The conclusion still holds. 
\end{eg}

\subsection{Analysis of Variance (ANOVA)}
\begin{defn}
\normalfont
Analysis of Variance (ANOVA) is a collection of statistical models and procedures for comparing factor level means in a factor. We wish to test if there is a statistically significant difference between the factor level/group means.
\end{defn}
One-Way ANOVA: One factor; Two-Way ANOVA: Two factors, etc.

It's called "Analysis of Variance" because we compare the \textbf{within-group} variance to the \textbf{between-group} variance. 

\textbf{Main Idea}: If the variation between groups is significantly bigger than the variation within groups, there is a statistically significant difference in the group means. 

Note: An ANOVA is equivalent to using a linear regression model with categorical predictors (with indicator variables).

\subsection{Recall: Multiple Linear Regression}
$$Y_i=\beta_0+\beta_1X_{1,i}+\beta_2X_{2,i}...+\beta_{p-1}X_{p-1,i}+\epsilon_i\text{ for }i=1,2,...,n$$
where
\begin{itemize}
\item $Y_i$: Response for the $i$th case (Quantitative variable)
\item $X_{1,i}...$: Predictors for $i$th case (Quantitative or categorical)
\item $\epsilon_i$: Error term for the $i$th case, where $\epsilon_i\stackrel{iid}{\sim}N(0, \sigma^2)$
\item $\beta_0, \beta_1, ..., \beta_{p-1}$: Regression coefficients/parameters, $\beta_0$: Intercept
\item $n$: Number of cases/sample size
\end{itemize}

Matrix form: $Y=X\beta+\epsilon$; where
\begin{itemize}
\item $Y$ is an $(n\times 1)$ vector
\item $X$ is $(n\times p)$ matrix, each column contains values for each predictor. If model uses intercept, first column of $X$ is all $1$s. 
\item $\beta$ is a $(p\times 1)$ vector of regression coefficients
\item $\epsilon$ is a $(n\times 1)$ vector of error terms
\end{itemize}
\begin{eg}
$y=\begin{pmatrix}
y_1\\
\vdots\\
y_n
\end{pmatrix}$, 
$X=\begin{pmatrix}
1 & x_{11} & x_{21} & ... & x_{p-1\ 1}\\
1 & x_{12} & x_{22} & ... & x_{p-1\ 2}\\
\vdots & \vdots & \vdots & \vdots & \vdots\\
1 & x_{1n} & x_{2n} & ... & x_{p-1\ n}
\end{pmatrix}$, 
$\beta=\begin{pmatrix}
\beta_0\\
\vdots\\
\beta_{p-1}
\end{pmatrix}$, 
$\epsilon=\begin{pmatrix}
\epsilon_1\\
\vdots\\
\epsilon_n
\end{pmatrix}$
\end{eg}
\subsection{Indicator Variables}
\begin{defn}
\normalfont
A factor has $a$ levels, when using a model with intercept, use only $(a-1)$ indicators. The level without an indicator is the default or reference level. For $k=1,2,...,a-1$
$$
I_{k,i}=
\begin{cases}
1, \text{\ if $i$th case belongs in factor level $k$}\\
0, \text{\ otherwise}
\end{cases}
$$
\end{defn}
\begin{eg}
\normalfont
\textbf{Linear Regression Model for Arthritis Example}
$$Y_i=\beta_0+\beta_1I_{A,i}+\epsilon_i\text{ for } i=1,2,...,n$$
where $Y_i=$ difference in pain after 6 months for $i$th patient. 
$$
E(Y_i)=
\begin{cases}
\beta_0+\beta_1, \text{\ if $i$th patient received Auranofin (1)}\\
\beta_0, \text{\ if $i$th patient received placebo (0)}
\end{cases}
$$

Therefore, \colorbox{pink}{$\beta_0$ is the mean difference in pain for placebo group}, \colorbox{pink}{$\beta_1$ is the difference in the mean} \colorbox{pink}{difference of pain (response) between Auranofin and Placebo groups. }

$\beta_1=0$: No difference

$\beta_1>0$: Mean in Auranofin group $>$ Placebo Group

$\beta_1<0$: Mean in Auranofin Group $<$ Placebo Group
\end{eg}

\subsection{One-Way ANOVA with 2 levels}
When there are 2 factor levels (comparing 2 means like in Arthritis Example), use t-test to test if $\beta_1=0$. Alternatively, use F-Test: Can be used to compare \colorbox{pink}{many} factor level means. T-Test using linear regression is \colorbox{pink}{equivalent} to F-Test using ANOVA when comparing 2 means. $t^2=F$. 
\newpage
\begin{eg}
\normalfont
Example of Arthritis Clinical Study- Linear Regression Model

\begin{center}
	\includegraphics[width=1\textwidth]{"pic 9".JPG}
\end{center}

\end{eg}

Hand Calculation: 

We have $H_0=\beta_1=0$, $H_a=\beta_1>0$, according to R output we have $t=2.64\sim t_{291}$ under $H_0$. 

\begin{center}
	\includegraphics[width=0.3\textwidth]{"pic 10".JPG}
\end{center}

$p=P(t_{291}>2.64)=\frac{0.00871}{2}\doteq 0.0044<0.05$. So, we have strong evidence to conclude that the Auranofin is effective in reducing pain. 

\subsection{Variation and Sum of Squares}
When we apply a treatment, we consider a possible treatment effect. In the two sample case, it is the difference between two groups in the population, i.e., $\mu_1-\mu_2$. This difference may not be due to the treatment effect alone, but experimental error as well. 

Generally, 

$H_0: \mu_1=\mu_2=...=\mu_a$ (No difference in group means, $a$ groups in total)

$H_a: \mu_i\neq \mu_j$ for at least one $i\neq j$ where $i,j=1,2,...,a$ (Difference in group means)

$$X=\frac{\text{variation between groups}}{\text{variation between subjects, within groups}}$$
If $X>1$, then more evidence we have a difference in group means (more significant). 

\subsection{One-Way ANOVA Notation}
\begin{itemize}
\item One factor, Factor A with $a$ levels
\item $j$ denotes the group/level and $i$ denotes the position of the subject within that level/group ($j=1,...,a$; $i=1,...,n_j$)
\item $y_{ij}$ denotes the $i$th observation from the $j$th group ($i$ is the index, $j$ is the group/level)
\item $n_j$ is the number of subjects in the $j$th group
\item \colorbox{pink}{$N=\sum_{j=1}^an_j$ is the total sample size}
\item $\bar{Y}_T$ is the grand sample mean regardless of group
\item $\bar{y}_j$ is the sample mean in the $j$th group
\end{itemize}

\begin{defn}
\normalfont
\textbf{Total Sum of Squares}

SST measures the \colorbox{pink}{total sample variability} (total deviation from the grand mean):
$$SST=\underbrace{\sum_{j=1}^a\sum_{i=1}^{n_j}}_{\text{All observations}}(y_{ij}-\bar{y}_T)^2$$
\begin{itemize}
\item $(N-1)$ df
\item $SST=SSReg+SSE$
\end{itemize}

\end{defn}

\begin{defn}
\normalfont
\textbf{Sum of Squares for A}

Sum of Squares for Factor A (SSA) measures the variability of the factor level means: 
$$SSA=\sum_{j=1}^a\sum_{i=1}^{n_j}(\hat{y}_{ij}-\bar{y}_T)^2=\sum_{j=1}^an_j(\bar{y}_j-\bar{y}_T)^2$$
Note that $\hat{y}_{ij}$ is the estimated observation, $\bar{y}_T$ is the grand sample mean. It has $(a-1)$ df, df=number of parameters in the model -1. It's normally called $SSA$ or $SSReg$. 

\end{defn}

\begin{defn}
\normalfont
\textbf{Residual/Error Sum of Squares}

SSE measures the variability that is left unexplained by the model: 
$$SSE=\sum_{j=1}^a\sum_{i=1}^{n_j}(y_{ij}-\hat{y}_{ij})^2=\sum_{j=1}^a\sum_{i=1}^{n_j}(y_{ij}-\bar{y}_j)^2$$
Note that it has $(N-a)$ df, df=number of data - number of parameters in the model. SSE decreases as more predictors are added to the model. We want SSE to be low! 
\end{defn}

\subsection{One-Way ANOVA Table}
Note that SS refers to Sum of Squares, MS refers to Mean Square = $\frac{SS}{df}$
\begin{center}
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{ANOVA Table} \\
 \hline
 Source & df &SS&MS\\
 \hline
 Group/Regression   & $a-1$    &$SSA$&   $\frac{SSA}{a-1}=MSA$\\
 Error &   $N-a$  & $SSE$   & $\frac{SSE}{N-a}=MSE$\\
 \hline
 Total &$N-1$ & $SST$ &  \\
 \hline
\end{tabular}
\end{center}
\vspace{18pt}

$$MSE=\frac{\sum_{j=1}^a\sum_{i=1}^{n_j}(y_{ij}-\bar{y}_j)^2}{N-a}=\frac{\sum_{j=1}^a(n_j-1)s_j^2}{N-a}=s_{\text{pooled}}^2$$

$SSA$ is often called ``between group" SS: measures the variability of observations between factor levels

$SSE$ is often called ``within group" SS: measures the variability of observations within factor levels. 
\newpage
\subsection{F-Test}
$$F=\frac{MSA}{MSE}\sim F_{a-1,N-a}$$
\textbf{Hypothesis Test Using ANOVA}

Hypothesis: $H_0:\mu_1=\mu_2=...=\mu_a$ vs. $H_a:$ at least one of the means $\mu_j$ differs from others; for $j=1,2,...,a$. 

Test Statistic and its Distribution under $H_0$: 
$$F=\frac{MSA}{MSE}=\frac{\frac{SSA}{a-1}}{\frac{SSE}{N-a}}\sim F_{a-1,N-a}\text{\ under }H_0$$
P-value: $p=P(F_{a-1,N-a}>F)$

\begin{eg}
\normalfont
\textbf{Arthiritis Clinical Study-ANOVA}

\begin{center}
	\includegraphics[width=1\textwidth]{"pic 11".JPG}
\end{center}

In this study we have levels $a=2$, $H_0: \mu_A=\mu_P$, $H_a: \mu_a\neq \mu_P$. 
$$F=\frac{MSA}{MSE}=\frac{MS_{\text{Treatment}}}{MS_{\text{Residual}}}=6.97\sim F_{1,291}$$
So the p-value will be $p=P(F_{1,291}>6.97)=0.008713<0.05$. Therefore, we reject $H_0$, we have very strong evidence that the Auranofin and Placebo therapy differs. Since $a=2$, we can have an easy follow up for two sample t-test with $H_a:\mu_A>\mu_P$. Once $\bar{y}_A>\bar{y}_P$ then we cam conclude that Auranofin is effective. 
\end{eg}

\section{Lec 3 One-Way ANOVA with $a$ Levels}
\subsection{Computational Formulae for SS}
\begin{itemize}
\item $SST=\sum_{j=1}^a\sum_{i=1}^{n_j}y_{ij}^2-N\bar{y}_T^2=[Y]-[T]$
\item $SSA=\sum_{j=1}^an_j\bar{y}_j^2-N\bar{y}_T^2=[A]-[T]$
\item $SSE=\sum_{j=1}^a\sum_{i=1}^{n_j}y_{ij}^2-\sum_{j=1}^an_j\bar{y}_j^2=[Y]-[A]$
\end{itemize}
$[Y]$ denotes the individual observations ($n$), $[A]$ denotes the actual groups means, $[T]$ denotes the grand means. The actual formula for these are:
\begin{itemize}
\item $[Y]=\sum_{j=1}^a\sum_{i=1}^{n_j}y_{ij}^2$
\item $[A]=\sum_{j=1}^an_j\bar{y}_j^2$
\item $[T]=N\bar{y}_T^2$
\end{itemize}
\subsection{Proofs of Computational Formulae for SS}
\begin{proof}

\begin{align*}
SST&=\sum_{j=1}^a\sum_{i=1}^{n_j}(y_{ij}-\bar{y}_T)^2\\
&=\sum_i\sum_jy_{ij}^2-2\bar{y}_T\sum_i\sum_jy_{ij}+\sum_i\sum_j\bar{y}_T^2\\
&=[Y]-2\bar{y}_TN\bar{y}_T+N\bar{y}_T^2\\
&=[Y]-N\bar{y}_T^2\\
&=[Y]-[T]
\end{align*}

\begin{align*}
SSA&=\sum_{j=1}^an_j(\bar{y}_j-\bar{y}_T)^2\\
&=\sum_jn_j\bar{y}_j^2-2\sum_j\bar{y}_Tn_j\bar{y}_j+\sum_jn_jy_T^2\\
&=[A]-2\bar{y}_T\sum_jn_j\bar{y}_j+Ny_T^2\\
&=[A]-N\bar{y}_T^2\\
&=[A]-[T]
\end{align*}

\begin{align*}
SSE&=\sum_i\sum_j(y_{ij}-\bar{y}_j)^2\\
&=\sum_i\sum_jy_{ij}^2-\sum_i\sum_j2\bar{y}_jy_{ij}+\sum_i\sum_j\bar{y}_j^2\\
&=[Y]-2\sum_j\bar{y}_j\sum_iy_{ij}+\sum_jn_j\bar{y}_j^2\\
&=[Y]-\sum_jn_j\bar{y}_j^2\\
&=[Y]-[A]
\end{align*}

\begin{align*}
SST&=[Y]-[T]\\
&=[A]-[T]+[Y]-[A]\\
&=SSA+SSE
\end{align*}

\end{proof}
\newpage
\begin{eg}
\normalfont

Example of Fertilizers in Farming

\begin{center}
	\includegraphics[width=0.8\textwidth]{"pic 12".JPG}
\end{center}
\newpage

R output as follows:

\begin{center}
	\includegraphics[width=0.8\textwidth]{"pic 13".JPG}
\end{center}

\begin{center}
	\includegraphics[width=0.8\textwidth]{"pic 14".JPG}
\end{center}

\begin{center}
	\includegraphics[width=0.8\textwidth]{"pic 15".JPG}
\end{center}

The experimental unit is a plot. The factor is the brand of fertilizer which are A, B, C, D (4 levels/treatments). The response is the crop field (in kg). In the experiment, $N=16$ plots, $n=4$, $n_A=n_C=n_C=n_D$ that means it's a balanced design. The hypothesis test is $H_0: \mu_A=\mu_B=\mu_C=\mu_D$ vs. $H_a: \mu_i\neq \mu_j$ for at least one pair of $i,j$ that $i\neq j$, and $i,j=$A, B, C, D. 

$$F=\frac{MS_{\text{Fertilizer}}}{MSE}=4.3629\sim F_{3,12} \text{ under } H_0$$
$$p=P(F_{3,12}>4.3629)\doteq 0.0269$$
Since p value is less than 0.05, reject $H_0$. Strong evidence to conclude that the brands of fertilizer defer in crop yield. 

Question for the next chapter:

Which fertilizer is the best? - We can examine the boxplot or look at all pairwise comparisons between fertilizers. 

Is Fertilizer A better than B, etc.? Does using Fertilizer A result in double the crop yield of using B, etc.? - Test Contrasts. 

\end{eg}

\subsection{What happens if we reject $H_0$?}
If we reject $H_0$, ANOVA tells us that there is statistically significant difference between group means. When there are more than 2 groups, it can be because: 
\begin{itemize}
\item One group has a different mean than others
\item Some groups have different means
\item All groups have different means from each other
\end{itemize}

\section{Lec 4 Follow-Up Comparisons for One-Way ANOVA: Testing Contrasts and Post-Hoc Analysis}
\subsection{Recall Example of Fertilizers in Farming- ANOVA}
\begin{eg}
\normalfont
The R output as follows: 

\begin{center}
	\includegraphics[width=0.8\textwidth]{"pic 16".JPG}
\end{center}
\end{eg}

\subsection{Pairwise Comparisons}
\begin{defn}
\normalfont
Compare one group mean to another group mean (1 df since there are 2 groups). Conducted like a two-sample t-test but use pooled estimate of variance which is calculated using all groups:
$$s_p^2=\frac{1}{N-a}\sum_{j=1}^a(n_j-1)s_j^2=MSE$$
where $s_j^2$ is the sample variance in the $j$th group and $\sum_{j=1}^an_j=N$. This $s_p^2$ is a better estimate of the variance than just using observations from 2 groups, which means more power. The idea is to set up (linear) contrasts to make comparisons. 
\end{defn}

\begin{defn}
\normalfont
\textbf{Linear Contrasts}
A linear contrast is a linear combination of the group means: $\psi=\sum_{j=1}^ac_j\mu_j$. Restrict sum of the coefficients in the contrast to be $0$: $\sum_{j=1}^ac_j=0$. 
\end{defn}
We use $H_0: \mu_j=\mu_k$ for $j\neq k$, $j$, $k=1,2,...,a$. Results in coefficients such as $c=\{1,-1,0,0\}$, or $c=\{0,1,0,-1\}$, etc. Usually make the sign of $\psi$ positive to reflect direction of the difference we predict. If group 1 is predicted to be higher than group 2, then $c=\{1,-1,0,0\}$ rather than $c=\{-1,1,0,0\}$. Test $\psi=0$ using $\psi=\sum_{j=1}^ac_j\bar{Y}_j$. 

Advantages of using Contrasts:
\begin{itemize}
\item Compare more than 2 groups or more than one set of equalities. 
\item General procedure that cover many possible research questions. 
\item Simple to check if contrasts are orthogonal/linearly independent. 
\end{itemize}

\begin{eg}
\normalfont
\textbf{Farming- Pairwise Comparisons Using Contrasts}

Set up a table with coefficients to compare these pairwise means:
\begin{itemize}
\item Is Fertilizer A different from B?- \textcolor{red}{$\psi_1$}
\item Is Fertilizer B different from C?- \textcolor{red}{$\psi_2$}
\item Is Fertilizer C different from D?- \textcolor{red}{$\psi_3$}
\end{itemize}

\begin{center}
\begin{tabular}{ |p{1cm}||p{1cm}|p{1cm}|p{1cm}|p{1cm}|  }
 \hline
 Group & A & B & C & D \\
 \hline
 $\psi_1$   & $1$    &$-1$&   $0$ & $0$\\
 $\psi_2$ &   $0$  & $1$   & $-1$ & $0$\\
 $\psi_3$ & $0$ & $0$ & $1$ & $-1$\\
 \hline
\end{tabular}
\end{center}
\newpage
\textbf{Table Including Means and Contrast Estimates}

\begin{center}
\begin{tabular}{ |c||c|c|c|c|c|  }
 \hline
 Group & A & B & C & D & Contrast Estimate\\
 \hline
 Means & $58.75$ & $60$ & $68.75$ & $65.5$ & $\hat{\psi}$\\
 \hline
 $\psi_1$   & $1$    &$-1$&   $0$ & $0$ & $-1.25$\\
 $\psi_2$ &   $0$  & $1$   & $-1$ & $0$ & $-8.75$\\
 $\psi_3$ & $0$ & $0$ & $1$ & $-1$ & $3.23$\\
 \hline
\end{tabular}
\end{center}

Example of the calculation: 
\begin{align*}
\hat{\psi}_1=\sum_{j=1}^4c_j\bar{y}_j&=1(\bar{y}_A)-1(\bar{y}_B)+0(\bar{y}_C)+0(\bar{y}_D)\\
&=\bar{y}_A-\bar{y}_B\\
&=58.75-60=-1.25
\end{align*}
\end{eg}

\begin{eg}
\normalfont
\textbf{Farming- More Complex Comparisons}

Set up a table with coefficients to compare the following: 
\begin{itemize}
\item Compare the two farms: \textcolor{red}{$\psi_4$}
\item Compare the two types of element-based fertilizers: \textcolor{red}{$\psi_5$}
\end{itemize}

\begin{center}
\begin{tabular}{ |c||c|c|c|c|  }
 \hline
 Farm & 2 & 2 & 1 & 1 \\
 Element & P & P & N & P\\
 \hline
 $\psi_4$   & $1$    &$1$&   $-1$ & $-1$\\
 $\psi_5$ &   $1$  & $1$   & $-3$ & $1$\\
 \hline
\end{tabular}
\end{center}

\textbf{Table Including All Comparisons}

\begin{center}
\begin{tabular}{ |c||c|c|c|c|c|  }
 \hline
 Group & A & B & C & D & Contrast Estimate\\
 \hline
 Farm & 2 & 2 & 1 & 1 & \\
 Element & P & P & N & P& \\
 Means & $58.75$ & $60$ & $68.75$ & $65.5$ & $\hat{\psi}$\\
 \hline
 $\psi_1$   & $1$    &$-1$&   $0$ & $0$ & $-1.25$\\
 $\psi_2$ &   $0$  & $1$   & $-1$ & $0$ & $-8.75$\\
 $\psi_3$ & $0$ & $0$ & $1$ & $-1$ & $3.23$\\
 $\psi_4$   & $1$    &$1$&   $-1$ & $-1$ & $-15.5$\\
 $\psi_5$ &   $1$  & $1$   & $-3$ & $1$ & $-22$\\
 \hline
\end{tabular}
\end{center}

Note that those numbers are table of $C_j$'s. 
\end{eg}

\subsection{Testing Using Contrasts}
Under certain conditions, $\hat{\psi}\sim N\left(\sum_{i=1}^ac_j\mu_j, \sigma^2\sum_{i=1}^a\frac{c_j^2}{n_j}\right)$
\begin{defn}
\normalfont
\textbf{Contrast Sum of Squares}

The Contrast Sum of Squares is explained variation by the contrast: 
$$SS_{\psi}=\frac{\hat{\psi}^2}{\sum_{j=1}^a(c_j^2/n_j)}$$
\begin{itemize}
\item Since 1 df, $MS_{\psi}=SS_{\psi}$
\item Compare to MSE to see if contrast explains more of the variation than noise:
$$F=\frac{MS_{\psi}}{MSE}$$
\item Equivalent to a t-test: $t=\frac{\hat{\psi}}{\sqrt{MSE\sum_{j=1}^a(c_j^2/n_j)}}\sim t_{N-a}$ under $H_0$
\item t Confidence Intervals for contrasts:
$$\hat{\psi}\pm t_{N-a;\alpha/2}\sqrt{MSE\sum_{j=1}^a(c_j^2/n_j)}$$
\end{itemize}
\end{defn}
\newpage
\begin{eg}
\normalfont
Farming R output: 

\begin{center}
	\includegraphics[width=0.8\textwidth]{"pic 17".JPG}
\end{center}

\begin{center}
	\includegraphics[width=0.8\textwidth]{"pic 18".JPG}
\end{center}

\begin{center}
	\includegraphics[width=0.8\textwidth]{"pic 19".JPG}
\end{center}

State the conclusions based on the contrasts that were tested. 
\begin{itemize}
\item Compare brands of fertilizers
\item Compare farms
\item Compare element bases
\item Which fertilizer brand would you recommend? Why?
\end{itemize}
\underline{Conclusions}: 
\begin{itemize}
\item Moderate (to strong) evidence that Fertilizer C yields more than Fertilizer B
\item Strong evidence that Farm 1 yields more than Farm 2
\item Moderate (to strong) evidence that Nitrogen-based fertilizers are better than Phosphorous-based.
\end{itemize}
\end{eg}

\subsection{Orthogonal Contrasts}
\begin{defn}
\normalfont
This is useful for dividing sums of squares into different components. Any SS can be decomposed into as many independent sums as there are degrees of freedom. Two contrasts are said to be \colorbox{pink}{orthogonal} if: for $\psi_1$ and $\psi_2$ with coefficient vectors $\vec{c_1}$ and $\vec{c_2}$ respectively, 
$$\sum_{j=1}^a\frac{c_{1j}c_{2j}}{n_j}=0$$
Note that when sample sizes equal, then orthogonality simplifies to dot product of $0$:
$$\vec{c_1}\cdot \vec{c_2}=\sum_{j=1}^ac_{1j}c_{2j}=0$$
Orthogonality is a property of a pair of contrasts. If there are $k$ contrasts, then there are ${k\choose 2}$ pairs. 
\end{defn}
\begin{defn}
\normalfont
If all contrasts in a set are orthogonal to one another, then it's called \textcolor{red}{mutually orthogonal}. There can be no more mutually orthogonal contrasts than the treatment df (i.e. $a-1$). Any set of $a-1$ mutually orthogonal contrasts can be used to express the SS for the treatment/factor (for balanced designs at least): 
$$SSA=\sum_{k=1}^{a-1}SS_{\psi k}$$
If orthogonal, each SS is independent and each contrast test can be interpreted additively (no problems with multi-collinearity). 
\end{defn}
\begin{eg}
\normalfont
\textbf{Example: Farming - Orthogonal Contrasts}

How many contrasts are orthogonal amongst the 5 contrasts we tested? 
\begin{itemize}
\item df for Fertilizer is 3. We are limited to 3 orthogonal contrasts
\item Can we find a set of 3? Which contrasts? 
\end{itemize}

In this example, we can easily find that $a=4$, df$=3$ (calculated from $a-1$)
\newpage

\begin{table}[h!]
\centering
\begin{tabular}{|c|c c c c|c|} 
\hline
\multicolumn{6}{|c|}{\textbf{Table to check Orthogonality}} \\
 \hline
 Group & A & B & C & D & \\ [0.5ex] 
 \hline
 $\psi_1$ & 1 & -1 & 0 & 0 &  \\ 
 $\psi_2$ & 0 & 1 & -1 & 0 &  \\
 $\psi_3$ & 0 & 0 & 1 & -1 &  \\
 $\psi_4$ & 1 & 1 & -1 & -1 &  \\
 $\psi_5$ & 1 & 1 & -3 & 1 & Sum \\ 
 \hline
 $\psi_1$ vs $\psi_2$ & 0 & -1 & 0 & 0 & -1 \\
  \textcolor{red}{$\psi_1$ vs $\psi_3$} & 0 & 0 & 0 & 0 &  \textcolor{red}{0} \\
  \textcolor{red}{$\psi_1$ vs $\psi_4$} & 1 & -1 & 0 & 0 &  \textcolor{red}{0} \\
 \textcolor{red}{$\psi_1$ vs $\psi_5$} & 1 & -1 & 0 & 0 &  \textcolor{red}{0} \\
 $\psi_2$ vs $\psi_3$ & 0 & 0 & -1 & 0 & -1 \\
 $\psi_2$ vs $\psi_4$ & 0 & 1 & 1 & 1 & 2 \\
 $\psi_2$ vs $\psi_5$ & 0 & 1 & 3 & 0 & 4 \\
 \textcolor{red}{$\psi_3$ vs $\psi_4$} & 0 & 0 & -1 & 1 & \textcolor{red}{0} \\
 $\psi_3$ vs $\psi_5$ & 0 & 0 & -3 & -1 & -4 \\
 $\psi_4$ vs $\psi_5$ & 1 & 1 & 3 & -1 & 4 \\
 \hline
\end{tabular}
\label{table:1}
\end{table}

We have ${5\choose 2}=10$ pairs of contrast. In this example, we have $a=4$, so there are $a-1=3$ mutually orthogonal contrasts. We will choose $\psi_1$, $\psi_3$, $\psi_4$ to be mutually orthogonal. 
\end{eg}
\subsection{Method for Constructing Mutually Orthogonal Set}
We have $a$ groups, we will compare 1st group to average of all other groups. For all other contrasts, make 1st group's coefficient 0. Compare 2nd group to average of the latter groups. Continue this pattern. Make 2nd group's coefficient 0. Compare 3rd group to average of the latter groups, and so on...

\textbf{Constructing Helmert Contrasts}

Start with $c_1=1$ and $c_j=-\frac{1}{a-1}$ for $j>1$. For $j=2,...,a-1$, let $c_k=0$ when $k\leq j-1$ and $c_j=1$ and $c_k=-\frac{1}{a-j}$ when $k\geq j+1$. These contrasts are known as \textcolor{red}{Helmert Contrasts}. 

\begin{eg}
\normalfont
\textbf{Farming: Helmert Contrasts}

Construct the Helmert Contrasts. Verify that their contrasts are mutually orthogonal. 

In this example, we have group $a=4$, it implies to there are $3$ contrasts are mutually orthogonal. Note that the following vector has the order of $(A, B, C, D)$:
$$\psi_1: \vec{c_1}=(1,-1/3, -1/3, -1/3)\implies A \text{ to average of others}$$
$$\psi_2: \vec{c_2}=(0, 1, -1/2, -1/2)\implies B \text{ to average of latter groups}$$
$$\psi_3: \vec{c_3}=(0, 0, 1, -1)\implies C \text{ to average of latter groups}$$
$$\vec{c_1}\cdot \vec{c_2}=\vec{c_1}\cdot \vec{c_3}=\vec{c_2}\cdot \vec{c_3}=0\implies \psi_1, \psi_2, \psi_3 \text{ are mutually orthogonal.} $$
\end{eg}

\subsection{Balanced Designs}
ANOVA is the best for balanced designs, there are potential problems with unbalanced designs. MSE in unbalanced designs:
\begin{itemize}
\item MSE is a pooled variance, weighted by size of each group. Larger groups dominate MSE towards their variance. 
\item MSE is still unbiased for $\sigma^2$. 
\end{itemize}
In balanced designs: 
\begin{itemize}
\item MSE gets an equal contribution from each group. 
\item SSA can be decomposed into the sum of the contrast SS. 
\item Usually more powerful. 
\end{itemize}
ANOVA can still be used for unbalanced designs. The more unequal the group sizes, the poorer the performance. So, we need to attempt to design balanced experiment. Sometimes necessary to use unbalanced designs (e.g. Larger control group). 

\subsection{Problem with Multiple Comparisons}
Inflation of Type I Error rate: Chance of making a Type I Error is very high when conducting many simultaneous tests. 

We are often interested in a family/group of different hypothesis tests or confidence intervals
\begin{itemize}
\item All pairwise differences in group means, there are ${a\choose 2}$ pairwise comparisons.
\item Meaningful contrasts
\end{itemize}
The \textcolor{red}{familywise error rate, $\alpha_{FW}$} is the probability of making at least one Type I Error in the set of tests. Suppose we conduct $k$ independent tests, each at significance level $\alpha$, then: 
\begin{align*}
\textcolor{red}{\alpha_{FW}}&\textcolor{red}{=P(\text{reject at least one $H_0|$all $H_0$ are true})}\\
&\textcolor{red}{=1-(1-\alpha)^k}
\end{align*}
\begin{eg}
\normalfont
\textbf{Familywise Type I Error Rate}

If we conduct $k=5$ tests (e.g., 5 contrasts such as in the Farming example) with $\alpha=0.05$, then
$$\alpha_{FW}=P(\text{at least one Type I Error})=1-(1-0.05)^5=0.2262$$
The probability of committing a Type I Error somewhere in these tests is now $22.62\%$. Inflated Type I Error: will increase even more as we increase number of tests. If tests are dependent, $\alpha_{FW}$ will be lower, but not lower than the Type I Error rate for any individual test. We need to control for this. 
\end{eg}

\subsection{Pre-planned Tests/Primary Research Questions}
Most often, an experiment is set up to answer one primary research question. Could also have follow-up questions. 
\begin{itemize}
\item If research question/tests are pre-planned/ad-hoc (before looking at the results of the analysis), it can be tested without correcting Type I Error
\item Why before and not after? \textit{Fishing for significant results!}
\item If question is important, do not conduct it in a family of less powerful tests
\item Need to correct if we test secondary research questions or make comparisons after seeing results of analysis (post-hoc)
\end{itemize}

\subsection{Simple Solution to Correct Type I Error}
\begin{itemize}
\item Decrease the family wise Type I Error rate ($\alpha_{FW}$) by decreasing the Type I Error rate for each individual test ($\alpha$)
\item Adjust Type I Error for the worst case (independent tests)
\item This adjustment results in less power/higher chance of making Type II Error
\end{itemize}
Are there other solutions? Yes: 
\begin{enumerate}
\item Bonferroni
\item Sidak-Bonferroni
\item Tukey's HSD
\item Scheffe
\end{enumerate}

\subsection{Bonferroni Method}
\begin{defn}
\normalfont
Bonferroni inequality: $P(A\cup B)\leq P(A)+P(B)$. 

Let $A_i$ be the event of making a Type I Error on $i$th test, then: $\alpha_{FW}=P(\bigcup_{i=1}^k A_i)\leq \sum_{i=1}^k P(A_i)$. Note that $\alpha_i=P(A_i)$ for $i=1,...,k$. 

\textbf{Bonferroni Method}

For each of the $k$ tests, use a significance level of $\alpha_{FW}/k$. Then CI coverage rate or Type I Error rate is at most $100(1-\alpha_{FW})\%$ or $\alpha_{FW}$. 
\begin{itemize}
\item Conservative procedure
\item Since the chance of making at least one Type I Error can be much lower than $\alpha$, it is likely that the $k$ tests may result in Type II Error (less powerful)
\item Type I Errors are not always more serious than Type II (depends on the context)
\end{itemize}
\end{defn}
\begin{eg}
\normalfont
\ 

\begin{center}
	\includegraphics[width=0.8\textwidth]{"pic 20".JPG}
\end{center}

\begin{center}
	\includegraphics[width=0.8\textwidth]{"pic 21".JPG}
\end{center}

\end{eg}
\subsection{Sidak-Bonferroni Procedure}
Bonderroni procedure is simple to use (by hand). We can improve procedure if using computer for calculations by directly working with: $\alpha_{FW}=1-(1-\alpha)^k$ 
\begin{defn}
\normalfont
\textbf{Sidak Bonderroni Method}

Adjust the individual Type I Error as follows: 
$$\alpha=1-(1-\alpha_{FW})^{1/k}$$
For example, $k=5$ and $\alpha_{FW}=0.05$ compare Bonferroni and Sidak-Bonferroni: 
\begin{itemize}
\item $\alpha_B=\frac{0.05}{5}=0.01$
\item $\alpha=1-(1-0.05)^{1/5}=0.0102$ This is slight higher due to lower individual Type I Error rate.
\end{itemize}
Still have same family-wise rate, but more power for individual tests.
\end{defn}
\newpage
\subsection{Confidence Intervals}
For tests, we adjust the p-values or individual significance levels. For CIs, adjust the critical value. Suppose we want to construct $k$ confidence intervals with familywise coverage of $100(1-\alpha_{FW})\%$, then use
$$\hat{\theta}\pm t_{df,\frac{\alpha_{FW}}{2k}}SE(\hat{\theta})$$
\subsection{Tukey's HSD Procedure}
\begin{itemize}
\item Based on ``Studentized Range Distribution" which is based on $\max_{j,k\in\{1,2,...,a\}}\{\bar{y}_j-\bar{y}_k\}$
\item Gives a simultaneous Type I Error rate of $\alpha_{FW}$ or confidence level of $100(1-\alpha_{FW})\%$
\item Conservative if unbalanced design
\item Less conservative than Bonferroni method, especially if group sample sizes are equal (approx.)
\end{itemize}
\begin{defn}
\normalfont
\textbf{Tukey's Honestly Significant Difference (for differences between pairs of groups)}

Confidence interval for $\mu_k-\mu_j$ is:
$$(\bar{y}_j-\bar{y}_k)\pm \frac{q_{\alpha_{FW};a,N-a}}{\sqrt{2}}\sqrt{MSE(\frac{1}{n_j}+\frac{1}{n_k})}$$
where $q_{\alpha;a,N-a}$ is the appropriate critical value of the Studentized Range Distribution. 

To avoid redundancy, compare the largest and smallest means and if we failed to reject $H_0$, then stop. If $H_0$ is rejected, compare next largest to next smallest means and continue on...
\end{defn}
Code: 
\begin{lstlisting}
> TukeyHSD(aov(model_name), factor=``factor_name")
\end{lstlisting}
`R'Gives CIs for all pairwise means. Reject $H_0$ for the given pair if the adjusted p-value $<\alpha$ or CI does not contain 0.



\end{document}



